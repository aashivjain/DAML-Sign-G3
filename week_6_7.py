# -*- coding: utf-8 -*-
"""Week_6_7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qiwFG6bzm31BpvGVRHc8YcoJkhSKJXWH
"""

from google.colab import files
uploaded = files.upload()  # upload kaggle.json

import os, shutil

os.makedirs("/root/.kaggle", exist_ok=True)
shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")
os.chmod("/root/.kaggle/kaggle.json", 0o600)

!kaggle datasets download -d grassknoted/asl-alphabet -p /content/data

import zipfile
zipfile.ZipFile('/content/data/asl-alphabet.zip').extractall('/content/data')

DATA_DIR = "/content/data/asl_alphabet_train/asl_alphabet_train"

import tensorflow as tf

IMG_SIZE = (224, 224)
BATCH_SIZE = 32

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATA_DIR,
    validation_split=0.4,
    subset="training",
    seed=42,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

valtest_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATA_DIR,
    validation_split=0.4,
    subset="validation",
    seed=42,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

val_size = 0.5

val_ds = valtest_ds.take(int(len(valtest_ds) * val_size))
test_ds = valtest_ds.skip(int(len(valtest_ds) * val_size))

from tensorflow.keras import layers

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(AUTOTUNE)
val_ds = val_ds.prefetch(AUTOTUNE)
test_ds = test_ds.prefetch(AUTOTUNE)

DATA_DIR = "/content/data/asl_alphabet_train/asl_alphabet_train"

import os
import cv2
import mediapipe as mp
import numpy as np
from tqdm import tqdm

mp_hands = mp.solutions.hands

def extract_landmarks_from_image(img_path):
    """Returns a (21,3) array or None if hand not detected."""
    img = cv2.imread(img_path)
    if img is None:
        return None

    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    with mp_hands.Hands(
        static_image_mode=True,
        max_num_hands=1,
        model_complexity=1
    ) as hands:
        results = hands.process(img_rgb)
        if not results.multi_hand_landmarks:
            return None

        lm = results.multi_hand_landmarks[0]
        coords = [[p.x, p.y, p.z] for p in lm.landmark]
        return np.array(coords)

def process_dataset(raw_dir, save_prefix, limit_per_class=200):
    X_raw, y_raw = [], []
    classes = sorted(os.listdir(raw_dir))

    for label in classes:
        class_folder = os.path.join(raw_dir, label)
        if not os.path.isdir(class_folder):
            continue

        print(f"Processing class: {label}")

        images = sorted(os.listdir(class_folder))[:limit_per_class]

        for img_name in tqdm(images, desc=label):
            img_path = os.path.join(class_folder, img_name)
            coords = extract_landmarks_from_image(img_path)
            if coords is not None:
                X_raw.append(coords)
                y_raw.append(label)

    os.makedirs("/content/processed", exist_ok=True)
    np.save("/content/processed/asl_X_raw.npy", np.array(X_raw, dtype=object))
    np.save("/content/processed/asl_y_raw.npy", np.array(y_raw))

    print("Saved:")
    print("/content/processed/asl_X_raw.npy")
    print("/content/processed/asl_y_raw.npy")

process_dataset(DATA_DIR, "/content/processed/asl", limit_per_class=200)

import numpy as np

def clean_landmark_array(coords):
    coords = np.asarray(coords, dtype=np.float64)

    coords = coords[:, :2]

    wrist = coords[0]
    coords = coords - wrist

    max_val = np.max(np.abs(coords))
    if max_val == 0:
        max_val = 1.0

    coords = coords / max_val

    return coords.flatten()

def clean_dataset():
    X_raw = np.load("/content/processed/asl_X_raw.npy", allow_pickle=True)
    y_raw = np.load("/content/processed/asl_y_raw.npy", allow_pickle=True)

    X_clean, y_clean = [], []

    for coords, lbl in zip(X_raw, y_raw):
        if coords.shape == (21, 3):
            cleaned = clean_landmark_array(coords)
            X_clean.append(cleaned)
            y_clean.append(lbl)

    X_clean = np.array(X_clean)
    y_clean = np.array(y_clean)

    np.save("/content/processed/asl_X_clean.npy", X_clean)
    np.save("/content/processed/asl_y_clean.npy", y_clean)

    print("Saved cleaned dataset:")
    print("/content/processed/asl_X_clean.npy")
    print("/content/processed/asl_y_clean.npy")

clean_dataset()

from sklearn.model_selection import train_test_split
import numpy as np

def prepare_splits():
    X = np.load("/content/processed/asl_X_clean.npy")
    y = np.load("/content/processed/asl_y_clean.npy")

    X_tv, X_test, y_tv, y_test = train_test_split(
        X, y, test_size=0.2, shuffle=True, stratify=y
    )

    val_ratio = 0.2 / 0.8
    X_train, X_val, y_train, y_val = train_test_split(
        X_tv, y_tv, test_size=val_ratio, shuffle=True, stratify=y_tv
    )

    np.save("/content/processed/asl_X_train.npy", X_train)
    np.save("/content/processed/asl_y_train.npy", y_train)
    np.save("/content/processed/asl_X_val.npy", X_val)
    np.save("/content/processed/asl_y_val.npy", y_val)
    np.save("/content/processed/asl_X_test.npy", X_test)
    np.save("/content/processed/asl_y_test.npy", y_test)

    print("Saved train/val/test splits.")

prepare_splits()

import numpy as np

X_train = np.load("/content/processed/asl_X_train.npy")
y_train = np.load("/content/processed/asl_y_train.npy")
X_val   = np.load("/content/processed/asl_X_val.npy")
y_val   = np.load("/content/processed/asl_y_val.npy")
X_test  = np.load("/content/processed/asl_X_test.npy")
y_test  = np.load("/content/processed/asl_y_test.npy")

X_train.shape, X_val.shape, X_test.shape

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

rf = RandomForestClassifier(
    n_estimators=400,
    max_depth=None,
    random_state=42
)

rf.fit(X_train, y_train)

preds = rf.predict(X_test)
rf_acc = accuracy_score(y_test, preds)
print("Random Forest Accuracy:", rf_acc)
print(classification_report(y_test, preds))

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

xgb = XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=8,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric="mlogloss"
)

xgb.fit(X_train, y_train_enc)

preds_enc = xgb.predict(X_test)

xgb_acc = accuracy_score(y_test_enc, preds_enc)
print("XGBoost Accuracy:", xgb_acc)

preds = le.inverse_transform(preds_enc)
print(classification_report(y_test, preds))

from sklearn.preprocessing import LabelEncoder
import numpy as np

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)

num_classes = len(le.classes_)
print("Classes:", le.classes_)
print("Num classes:", num_classes)

import tensorflow as tf
from tensorflow.keras import layers, models

mlp = models.Sequential([
    layers.Input(shape=(42,)),
    layers.Dense(128, activation='relu'),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(num_classes, activation='softmax')
])

mlp.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history = mlp.fit(
    X_train, y_train_enc,
    validation_data=(X_val, y_val_enc),
    epochs=20,
    batch_size=32
)

mlp_test_loss, mlp_test_acc = mlp.evaluate(X_test, y_test_enc)
print("MLP Test Accuracy:", mlp_test_acc)

from sklearn.metrics import classification_report

preds_enc = mlp.predict(X_test).argmax(axis=1)
preds = le.inverse_transform(preds_enc)

print(classification_report(y_test, preds))

import matplotlib.pyplot as plt

plt.plot(history.history["accuracy"], label="Train")
plt.plot(history.history["val_accuracy"], label="Val")
plt.legend()
plt.title("MLP Accuracy Curve")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.show()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)
num_classes = len(le.classes_)

import tensorflow as tf
from tensorflow.keras import layers, models

X_train_lstm = X_train.reshape(-1, 21, 2)
X_val_lstm   = X_val.reshape(-1, 21, 2)
X_test_lstm  = X_test.reshape(-1, 21, 2)

lstm_model = models.Sequential([
    layers.LSTM(64, return_sequences=True),
    layers.LSTM(64),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])

lstm_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

lstm_model.fit(
    X_train_lstm, y_train_enc,
    validation_data=(X_val_lstm, y_val_enc),
    epochs=15,
    batch_size=32
)

lstm_test_loss, lstm_test_acc = lstm_model.evaluate(X_test_lstm, y_test_enc)
print("LSTM Test Accuracy:", lstm_test_acc)

from sklearn.metrics import classification_report
import numpy as np

preds_enc = lstm_model.predict(X_test_lstm).argmax(axis=1)
preds = le.inverse_transform(preds_enc)

print(classification_report(y_test, preds))

print("Random Forest:", rf_acc)
print("XGBoost:", xgb_acc)
print("MLP:", mlp_test_acc)

if 'lstm_test_acc' in globals():
    print("LSTM:", lstm_test_acc)

!pip install seaborn

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.decomposition import PCA

results = {}

if 'rf_acc' in globals():
    results['Random Forest'] = rf_acc
if 'xgb_acc' in globals():
    results['XGBoost'] = xgb_acc
if 'mlp_test_acc' in globals():
    results['MLP'] = mlp_test_acc
if 'lstm_test_acc' in globals():
    results['LSTM'] = lstm_test_acc

model_names = list(results.keys())
accuracies = [results[m] for m in model_names]

plt.figure(figsize=(7,5))
bars = plt.bar(model_names, accuracies)
plt.ylim(0, 1.0)
plt.ylabel("Accuracy")
plt.title("Model Accuracy Comparison on ASL Landmark Data")

for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f"{acc:.3f}", ha='center', va='bottom')

plt.show()

mlp_preds_enc = mlp.predict(X_test).argmax(axis=1)

cm = confusion_matrix(y_test_enc, mlp_preds_enc)
classes = le.classes_

plt.figure(figsize=(12,10))
sns.heatmap(cm,
            cmap="Blues",
            xticklabels=classes,
            yticklabels=classes,
            cbar_kws={'label': 'Count'})
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix – MLP on ASL Landmarks")
plt.tight_layout()
plt.show()

cm_sums = cm.sum(axis=1)
per_class_acc = cm.diagonal() / cm_sums

plt.figure(figsize=(12,5))
plt.bar(classes, per_class_acc)
plt.xticks(rotation=45)
plt.ylim(0, 1.0)
plt.ylabel("Accuracy")
plt.title("Per-Class Accuracy – MLP")
for x, acc in zip(range(len(classes)), per_class_acc):
    plt.text(x, acc + 0.01, f"{acc:.2f}", ha='center', va='bottom', fontsize=8)
plt.tight_layout()
plt.show()

rf_importances = rf.feature_importances_

landmark_importance = []
for i in range(21):
    landmark_importance.append(rf_importances[2*i:2*i+2].sum())
landmark_importance = np.array(landmark_importance)

landmark_names = [
    "WRIST",
    "THUMB_CMC", "THUMB_MCP", "THUMB_IP", "THUMB_TIP",
    "INDEX_MCP", "INDEX_PIP", "INDEX_DIP", "INDEX_TIP",
    "MIDDLE_MCP", "MIDDLE_PIP", "MIDDLE_DIP", "MIDDLE_TIP",
    "RING_MCP", "RING_PIP", "RING_DIP", "RING_TIP",
    "PINKY_MCP", "PINKY_PIP", "PINKY_DIP", "PINKY_TIP"
]

order = np.argsort(landmark_importance)[::-1]

plt.figure(figsize=(8,7))
plt.barh(np.array(landmark_names)[order], landmark_importance[order])
plt.gca().invert_yaxis()
plt.xlabel("Importance (Random Forest)")
plt.title("Most Important Hand Landmarks for Classification")
plt.tight_layout()
plt.show()

X_all = np.concatenate([X_train, X_val, X_test], axis=0)

mean_coords = X_all.mean(axis=0)  # length 42
xs = mean_coords[0::2]
ys = mean_coords[1::2]

imp = landmark_importance
imp_norm = (imp - imp.min()) / (imp.max() - imp.min() + 1e-8)

plt.figure(figsize=(5,6))
scatter = plt.scatter(xs, -ys, c=imp_norm, s=200, cmap="Reds")

for i, name in enumerate(landmark_names):
    plt.text(xs[i], -ys[i]+0.03, name.split("_")[0], ha='center', fontsize=7)

plt.colorbar(scatter, label="Relative Importance")
plt.title("Hand Landmark Importance Heatmap")
plt.axis('off')
plt.tight_layout()
plt.show()

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_test_2d = pca.fit_transform(X_test)

plt.figure(figsize=(8,6))
scatter = plt.scatter(X_test_2d[:,0], X_test_2d[:,1],
                      c=y_test_enc, cmap="tab20", s=10, alpha=0.7)
plt.title("PCA of ASL Landmark Features (Test Set)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.colorbar(scatter, label="Class index")
plt.tight_layout()
plt.show()

import random
import cv2

def random_brightness_contrast(img, alpha_range=(0.6, 1.4), beta_range=(-30, 30)):
    alpha = np.random.uniform(*alpha_range)  # contrast
    beta = np.random.uniform(*beta_range)    # brightness
    new = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)
    return new

num_samples = 300
indices = np.random.choice(len(y_test), size=num_samples, replace=False)

original_correct = 0
augmented_correct = 0

print("=== MLP Classification Report ===")
print(classification_report(
    y_test,
    le.inverse_transform(mlp.predict(X_test).argmax(axis=1)),
    digits=3
))

from sklearn.preprocessing import LabelEncoder
import joblib

mlp.save("asl_landmark_mlp.h5")

joblib.dump(le, "label_encoder.joblib")

clean_landmark_code = """
import numpy as np

def clean_landmark_array(coords):
    coords = np.asarray(coords, dtype=np.float64)
    coords = coords[:, :2]          # drop z
    wrist = coords[0]               # center on wrist
    coords = coords - wrist
    max_val = np.max(np.abs(coords))
    if max_val == 0:
        max_val = 1.0
    coords = coords / max_val
    return coords.flatten()
"""

with open("preprocessing_utils.py", "w") as f:
    f.write(clean_landmark_code)

print("Saved: asl_landmark_mlp.h5, label_encoder.joblib, preprocessing_utils.py")

import numpy as np
import cv2
import mediapipe as mp
import joblib
import matplotlib.pyplot as plt

from tensorflow.keras.models import load_model
from sklearn.preprocessing import LabelEncoder

from google.colab import output
from google.colab.output import eval_js
from IPython.display import Javascript, display
from base64 import b64decode
from io import BytesIO
from PIL import Image

mlp = load_model("/content/asl_landmark_mlp.h5")
le = joblib.load("/content/label_encoder.joblib")

class_names = list(le.classes_)
num_classes = len(class_names)
print("Loaded classes:", class_names)

def clean_landmark_array(coords):
    """
    coords: (21,3) array from Mediapipe
    returns: (42,) normalized feature vector
    """
    coords = np.asarray(coords, dtype=np.float64)
    coords = coords[:, :2]
    wrist = coords[0]
    coords = coords - wrist
    max_val = np.max(np.abs(coords))
    if max_val == 0:
        max_val = 1.0
    coords = coords / max_val
    return coords.flatten()

def take_photo(quality=0.8):
    js = Javascript('''
      async function takePhoto(quality) {
        const div = document.createElement('div');
        const video = document.createElement('video');
        const btn = document.createElement('button');
        const span = document.createElement('span');
        span.textContent = "Click capture when ready";
        btn.textContent = 'Capture';
        div.appendChild(span);
        div.appendChild(document.createElement('br'));
        div.appendChild(video);
        div.appendChild(document.createElement('br'));
        div.appendChild(btn);
        document.body.appendChild(div);

        const stream = await navigator.mediaDevices.getUserMedia({video: true});
        video.srcObject = stream;
        await video.play();

        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        await new Promise((resolve) => btn.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);
        stream.getVideoTracks()[0].stop();
        div.remove();

        return canvas.toDataURL('image/jpeg', quality);
      }
    ''')
    display(js)
    data = eval_js('takePhoto({})'.format(quality))
    binary = b64decode(data.split(',')[1])
    img = Image.open(BytesIO(binary))
    return np.array(img)  # RGB numpy array

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

def predict_letter_from_image(rgb_img):
    """
    rgb_img: numpy array (H,W,3) in RGB
    returns (predicted_label or None, annotated_bgr_img)
    """
    with mp_hands.Hands(
        static_image_mode=True,
        max_num_hands=1,
        model_complexity=1
    ) as hands:

        results = hands.process(rgb_img)
        bgr_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2BGR)

        if not results.multi_hand_landmarks:
            return None, bgr_img

        hand_landmarks = results.multi_hand_landmarks[0]
        coords = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])

        mp_drawing.draw_landmarks(
            bgr_img, hand_landmarks, mp_hands.HAND_CONNECTIONS
        )

        features = clean_landmark_array(coords).reshape(1, -1)
        probs = mlp.predict(features, verbose=0)[0]
        idx = np.argmax(probs)
        label = le.inverse_transform([idx])[0]

        return label, bgr_img

current_phrase = ""

def capture_and_predict_phrase():
    global current_phrase

    rgb = take_photo()

    label, annotated_bgr = predict_letter_from_image(rgb)

    annotated_rgb = cv2.cvtColor(annotated_bgr, cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(6,4))
    plt.imshow(annotated_rgb)
    if label is None:
        title = "No hand detected – try again"
    else:
        current_phrase += label
        title = f"Predicted letter: {label} | Phrase so far: {current_phrase}"
    plt.title(title)
    plt.axis('off')
    plt.show()

    print("Phrase so far:", current_phrase)

def clear_phrase():
    global current_phrase
    current_phrase = ""
    print("Phrase cleared.")

clear_phrase()

capture_and_predict_phrase()