# -*- coding: utf-8 -*-
"""Week_6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qiwFG6bzm31BpvGVRHc8YcoJkhSKJXWH
"""

from google.colab import files
uploaded = files.upload()  # upload kaggle.json

import os, shutil

os.makedirs("/root/.kaggle", exist_ok=True)
shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")
os.chmod("/root/.kaggle/kaggle.json", 0o600)

!kaggle datasets download -d grassknoted/asl-alphabet -p /content/data

import zipfile
zipfile.ZipFile('/content/data/asl-alphabet.zip').extractall('/content/data')

DATA_DIR = "/content/data/asl_alphabet_train/asl_alphabet_train"

import tensorflow as tf

IMG_SIZE = (224, 224)
BATCH_SIZE = 32

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATA_DIR,
    validation_split=0.4,  # 40% is val+test
    subset="training",
    seed=42,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

valtest_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATA_DIR,
    validation_split=0.4,
    subset="validation",
    seed=42,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

val_size = 0.5  # half of the 40% = 20%

val_ds = valtest_ds.take(int(len(valtest_ds) * val_size))
test_ds = valtest_ds.skip(int(len(valtest_ds) * val_size))

from tensorflow.keras import layers

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(AUTOTUNE)
val_ds = val_ds.prefetch(AUTOTUNE)
test_ds = test_ds.prefetch(AUTOTUNE)

DATA_DIR = "/content/data/asl_alphabet_train/asl_alphabet_train"

!pip install mediapipe

import os
import cv2
import mediapipe as mp
import numpy as np
from tqdm import tqdm

mp_hands = mp.solutions.hands

def extract_landmarks_from_image(img_path):
    """Returns a (21,3) array or None if hand not detected."""
    img = cv2.imread(img_path)
    if img is None:
        return None

    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    with mp_hands.Hands(
        static_image_mode=True,
        max_num_hands=1,
        model_complexity=1
    ) as hands:
        results = hands.process(img_rgb)
        if not results.multi_hand_landmarks:
            return None

        lm = results.multi_hand_landmarks[0]
        coords = [[p.x, p.y, p.z] for p in lm.landmark]
        return np.array(coords)

def process_dataset(raw_dir, save_prefix, limit_per_class=200):
    X_raw, y_raw = [], []
    classes = sorted(os.listdir(raw_dir))

    for label in classes:
        class_folder = os.path.join(raw_dir, label)
        if not os.path.isdir(class_folder):
            continue

        print(f"Processing class: {label}")

        images = sorted(os.listdir(class_folder))[:limit_per_class]

        for img_name in tqdm(images, desc=label):
            img_path = os.path.join(class_folder, img_name)
            coords = extract_landmarks_from_image(img_path)
            if coords is not None:  # keep only valid detections
                X_raw.append(coords)
                y_raw.append(label)

    # Save raw arrays
    os.makedirs("/content/processed", exist_ok=True)
    np.save("/content/processed/asl_X_raw.npy", np.array(X_raw, dtype=object))
    np.save("/content/processed/asl_y_raw.npy", np.array(y_raw))

    print("Saved:")
    print("/content/processed/asl_X_raw.npy")
    print("/content/processed/asl_y_raw.npy")

process_dataset(DATA_DIR, "/content/processed/asl", limit_per_class=200)

import numpy as np

def clean_landmark_array(coords):
    coords = np.asarray(coords, dtype=np.float64)   # shape (21,3)

    # Remove z-axis
    coords = coords[:, :2]

    # Center at wrist (landmark 0)
    wrist = coords[0]
    coords = coords - wrist

    # Normalize by max abs value
    max_val = np.max(np.abs(coords))
    if max_val == 0:
        max_val = 1.0

    coords = coords / max_val

    # Flatten â†’ 42 dims
    return coords.flatten()

def clean_dataset():
    X_raw = np.load("/content/processed/asl_X_raw.npy", allow_pickle=True)
    y_raw = np.load("/content/processed/asl_y_raw.npy", allow_pickle=True)

    X_clean, y_clean = [], []

    for coords, lbl in zip(X_raw, y_raw):
        if coords.shape == (21, 3):
            cleaned = clean_landmark_array(coords)
            X_clean.append(cleaned)
            y_clean.append(lbl)

    X_clean = np.array(X_clean)
    y_clean = np.array(y_clean)

    np.save("/content/processed/asl_X_clean.npy", X_clean)
    np.save("/content/processed/asl_y_clean.npy", y_clean)

    print("Saved cleaned dataset:")
    print("/content/processed/asl_X_clean.npy")
    print("/content/processed/asl_y_clean.npy")

clean_dataset()

from sklearn.model_selection import train_test_split
import numpy as np

def prepare_splits():
    X = np.load("/content/processed/asl_X_clean.npy")
    y = np.load("/content/processed/asl_y_clean.npy")

    # Train+Val vs Test
    X_tv, X_test, y_tv, y_test = train_test_split(
        X, y, test_size=0.2, shuffle=True, stratify=y
    )

    # Train vs Val
    val_ratio = 0.2 / 0.8
    X_train, X_val, y_train, y_val = train_test_split(
        X_tv, y_tv, test_size=val_ratio, shuffle=True, stratify=y_tv
    )

    np.save("/content/processed/asl_X_train.npy", X_train)
    np.save("/content/processed/asl_y_train.npy", y_train)
    np.save("/content/processed/asl_X_val.npy", X_val)
    np.save("/content/processed/asl_y_val.npy", y_val)
    np.save("/content/processed/asl_X_test.npy", X_test)
    np.save("/content/processed/asl_y_test.npy", y_test)

    print("Saved train/val/test splits.")

prepare_splits()

import numpy as np

X_train = np.load("/content/processed/asl_X_train.npy")
y_train = np.load("/content/processed/asl_y_train.npy")
X_val   = np.load("/content/processed/asl_X_val.npy")
y_val   = np.load("/content/processed/asl_y_val.npy")
X_test  = np.load("/content/processed/asl_X_test.npy")
y_test  = np.load("/content/processed/asl_y_test.npy")

X_train.shape, X_val.shape, X_test.shape

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

rf = RandomForestClassifier(
    n_estimators=400,
    max_depth=None,
    random_state=42
)

rf.fit(X_train, y_train)

preds = rf.predict(X_test)
rf_acc = accuracy_score(y_test, preds)
print("Random Forest Accuracy:", rf_acc)
print(classification_report(y_test, preds))

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

xgb = XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=8,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric="mlogloss"
)

# IMPORTANT: use y_train_enc here
xgb.fit(X_train, y_train_enc)

# Predict encoded labels
preds_enc = xgb.predict(X_test)

# Accuracy in encoded space
xgb_acc = accuracy_score(y_test_enc, preds_enc)
print("XGBoost Accuracy:", xgb_acc)

# Convert predictions back to original string labels for a nice report
preds = le.inverse_transform(preds_enc)
print(classification_report(y_test, preds))

from sklearn.preprocessing import LabelEncoder
import numpy as np

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)

num_classes = len(le.classes_)
print("Classes:", le.classes_)
print("Num classes:", num_classes)

import tensorflow as tf
from tensorflow.keras import layers, models

mlp = models.Sequential([
    layers.Input(shape=(42,)),
    layers.Dense(128, activation='relu'),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(num_classes, activation='softmax')
])

mlp.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history = mlp.fit(
    X_train, y_train_enc,
    validation_data=(X_val, y_val_enc),
    epochs=20,
    batch_size=32
)

mlp_test_loss, mlp_test_acc = mlp.evaluate(X_test, y_test_enc)
print("MLP Test Accuracy:", mlp_test_acc)

from sklearn.metrics import classification_report

preds_enc = mlp.predict(X_test).argmax(axis=1)
preds = le.inverse_transform(preds_enc)

print(classification_report(y_test, preds))

import matplotlib.pyplot as plt

plt.plot(history.history["accuracy"], label="Train")
plt.plot(history.history["val_accuracy"], label="Val")
plt.legend()
plt.title("MLP Accuracy Curve")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.show()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)
num_classes = len(le.classes_)

import tensorflow as tf
from tensorflow.keras import layers, models

# reshaping required: (samples, timesteps=21, features=2)
X_train_lstm = X_train.reshape(-1, 21, 2)
X_val_lstm   = X_val.reshape(-1, 21, 2)
X_test_lstm  = X_test.reshape(-1, 21, 2)

lstm_model = models.Sequential([
    layers.LSTM(64, return_sequences=True),
    layers.LSTM(64),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes, activation='softmax')   # <- use num_classes, not np.unique(y_train)
])

lstm_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

lstm_model.fit(
    X_train_lstm, y_train_enc,               # <- use encoded labels
    validation_data=(X_val_lstm, y_val_enc),
    epochs=15,
    batch_size=32
)

lstm_test_loss, lstm_test_acc = lstm_model.evaluate(X_test_lstm, y_test_enc)
print("LSTM Test Accuracy:", lstm_test_acc)

from sklearn.metrics import classification_report
import numpy as np

preds_enc = lstm_model.predict(X_test_lstm).argmax(axis=1)
preds = le.inverse_transform(preds_enc)

print(classification_report(y_test, preds))

print("Random Forest:", rf_acc)
print("XGBoost:", xgb_acc)
print("MLP:", mlp_test_acc)

if 'lstm_test_acc' in globals():
    print("LSTM:", lstm_test_acc)