# -*- coding: utf-8 -*-
"""Week 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xLCpnFx1-rc1uINBLcJCgNi4TXkvyMMw
"""

from google.colab import files
uploaded = files.upload()  # upload kaggle.json

import os, shutil

os.makedirs("/root/.kaggle", exist_ok=True)
shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")
os.chmod("/root/.kaggle/kaggle.json", 0o600)

!kaggle datasets download -d grassknoted/asl-alphabet -p /content/data

import zipfile
zipfile.ZipFile('/content/data/asl-alphabet.zip').extractall('/content/data')

DATA_DIR = "/content/data/asl_alphabet_train/asl_alphabet_train"

import tensorflow as tf

IMG_SIZE = (224, 224)
BATCH_SIZE = 32

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATA_DIR,
    validation_split=0.4,  # 40% is val+test
    subset="training",
    seed=42,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

valtest_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATA_DIR,
    validation_split=0.4,
    subset="validation",
    seed=42,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

val_size = 0.5  # half of the 40% = 20%

val_ds = valtest_ds.take(int(len(valtest_ds) * val_size))
test_ds = valtest_ds.skip(int(len(valtest_ds) * val_size))

from tensorflow.keras import layers

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(AUTOTUNE)
val_ds = val_ds.prefetch(AUTOTUNE)
test_ds = test_ds.prefetch(AUTOTUNE)

DATA_DIR = "/content/data/asl_alphabet_train/asl_alphabet_train"

import os
import cv2
import mediapipe as mp
import numpy as np
from tqdm import tqdm

mp_hands = mp.solutions.hands

def extract_landmarks_from_image(img_path):
    """Returns a (21,3) array or None if hand not detected."""
    img = cv2.imread(img_path)
    if img is None:
        return None

    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    with mp_hands.Hands(
        static_image_mode=True,
        max_num_hands=1,
        model_complexity=1
    ) as hands:
        results = hands.process(img_rgb)
        if not results.multi_hand_landmarks:
            return None

        lm = results.multi_hand_landmarks[0]
        coords = [[p.x, p.y, p.z] for p in lm.landmark]
        return np.array(coords)

def process_dataset(raw_dir, save_prefix, limit_per_class=200):
    X_raw, y_raw = [], []
    classes = sorted(os.listdir(raw_dir))

    for label in classes:
        class_folder = os.path.join(raw_dir, label)
        if not os.path.isdir(class_folder):
            continue

        print(f"Processing class: {label}")

        images = sorted(os.listdir(class_folder))[:limit_per_class]

        for img_name in tqdm(images, desc=label):
            img_path = os.path.join(class_folder, img_name)
            coords = extract_landmarks_from_image(img_path)
            if coords is not None:  # keep only valid detections
                X_raw.append(coords)
                y_raw.append(label)

    # Save raw arrays
    os.makedirs("/content/processed", exist_ok=True)
    np.save("/content/processed/asl_X_raw.npy", np.array(X_raw, dtype=object))
    np.save("/content/processed/asl_y_raw.npy", np.array(y_raw))

    print("Saved:")
    print("/content/processed/asl_X_raw.npy")
    print("/content/processed/asl_y_raw.npy")

process_dataset(DATA_DIR, "/content/processed/asl", limit_per_class=200)

import numpy as np

def clean_landmark_array(coords):
    coords = np.asarray(coords, dtype=np.float64)   # shape (21,3)

    # Remove z-axis
    coords = coords[:, :2]

    # Center at wrist (landmark 0)
    wrist = coords[0]
    coords = coords - wrist

    # Normalize by max abs value
    max_val = np.max(np.abs(coords))
    if max_val == 0:
        max_val = 1.0

    coords = coords / max_val

    # Flatten â†’ 42 dims
    return coords.flatten()

def clean_dataset():
    X_raw = np.load("/content/processed/asl_X_raw.npy", allow_pickle=True)
    y_raw = np.load("/content/processed/asl_y_raw.npy", allow_pickle=True)

    X_clean, y_clean = [], []

    for coords, lbl in zip(X_raw, y_raw):
        if coords.shape == (21, 3):
            cleaned = clean_landmark_array(coords)
            X_clean.append(cleaned)
            y_clean.append(lbl)

    X_clean = np.array(X_clean)
    y_clean = np.array(y_clean)

    np.save("/content/processed/asl_X_clean.npy", X_clean)
    np.save("/content/processed/asl_y_clean.npy", y_clean)

    print("Saved cleaned dataset:")
    print("/content/processed/asl_X_clean.npy")
    print("/content/processed/asl_y_clean.npy")

clean_dataset()

from sklearn.model_selection import train_test_split
import numpy as np

def prepare_splits():
    X = np.load("/content/processed/asl_X_clean.npy")
    y = np.load("/content/processed/asl_y_clean.npy")

    # Train+Val vs Test
    X_tv, X_test, y_tv, y_test = train_test_split(
        X, y, test_size=0.2, shuffle=True, stratify=y
    )

    # Train vs Val
    val_ratio = 0.2 / 0.8
    X_train, X_val, y_train, y_val = train_test_split(
        X_tv, y_tv, test_size=val_ratio, shuffle=True, stratify=y_tv
    )

    np.save("/content/processed/asl_X_train.npy", X_train)
    np.save("/content/processed/asl_y_train.npy", y_train)
    np.save("/content/processed/asl_X_val.npy", X_val)
    np.save("/content/processed/asl_y_val.npy", y_val)
    np.save("/content/processed/asl_X_test.npy", X_test)
    np.save("/content/processed/asl_y_test.npy", y_test)

    print("Saved train/val/test splits.")

prepare_splits()

!pip install tensorflow matplotlib numpy

import tensorflow as tf

IMG_SIZE = (224, 224)
BATCH_SIZE = 32
SEED = 42

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATA_DIR,
    validation_split=0.2,
    subset="training",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATA_DIR,
    validation_split=0.2,
    subset="validation",
    seed=SEED,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE
)

class_names = train_ds.class_names
num_classes = len(class_names)
num_classes

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(AUTOTUNE)
val_ds = val_ds.prefetch(AUTOTUNE)

from tensorflow.keras import layers, models

base_model = tf.keras.applications.MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,
    weights="imagenet"
)

base_model.trainable = False  # freeze backbone

inputs = tf.keras.Input(shape=(224, 224, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)
x = base_model(x, training=False)

x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.2)(x)

outputs = layers.Dense(num_classes, activation="softmax")(x)
model = models.Model(inputs, outputs)

model.summary()

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10
)

base_model.trainable = True

for layer in base_model.layers[:-20]:
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history_fine = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=5
)

import matplotlib.pyplot as plt

acc = history.history['accuracy'] + history_fine.history['accuracy']
val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']

plt.figure(figsize=(8,6))
plt.plot(acc, label='Train Accuracy')
plt.plot(val_acc, label='Val Accuracy')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Training & Validation Accuracy")
plt.legend()
plt.show()

loss, acc = model.evaluate(val_ds)
print("Final Validation Accuracy:", acc)

model.save("/content/asl_transfer_model.h5")